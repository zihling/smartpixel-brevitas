{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c7f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.model import (\n",
    "    QuantDenseModel,\n",
    "    IntQuantDenseModel,\n",
    "    IntQuantDenseModelLarge,\n",
    "    FloatQuantDenseModel,\n",
    "    FloatQuantDenseModelLarge,\n",
    ")\n",
    "from utils.train import (\n",
    "    train_for_epoch,\n",
    "    test_model,\n",
    ")\n",
    "from utils.dataset import (\n",
    "    get_smartpixel_dataloaders,\n",
    "    get_global_smartpixel_dataloaders,   \n",
    ")\n",
    "\n",
    "DATASET_PATH = \"./data/ds8_only/dec6_ds8_quant\"  # Change me\n",
    "EXPORT_PATH = \"./model_outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77463b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "# momentum = 0.5\n",
    "\n",
    "# ++++ Quantization method (comment/uncomment) ++++\n",
    "# Int quant\n",
    "# model = IntQuantDenseModel(in_features=13, dense_width=16, logit_total_bits=3, activation_total_bits=6, num_classes=12) # Small model\n",
    "\n",
    "# model = IntQuantDenseModel(in_features=13, dense_width=58, logit_total_bits=4, activation_total_bits=8, num_classes=12) # Medium model\n",
    "\n",
    "# model = IntQuantDenseModel(in_features=13, dense_width=512, logit_total_bits=4, activation_total_bits=8, num_classes=12) # Large model2\n",
    "\n",
    "# model = IntQuantDenseModelLarge(in_features=13, dense_width=32, logit_total_bits=8, activation_total_bits=10, num_classes=12) # Large model\n",
    "\n",
    "\n",
    "# Minifloat quant\n",
    "# model = FloatQuantDenseModel(\n",
    "#     in_features=13,\n",
    "#     dense_width=16,\n",
    "#     num_classes=12,\n",
    "#     input_exponent_bit_width=4,\n",
    "#     input_mantissa_bit_width=3,\n",
    "#     weight_exponent_bit_width=4,\n",
    "#     weight_mantissa_bit_width=3,\n",
    "# ) # !: Small(or medium) model\n",
    "\n",
    "model = FloatQuantDenseModelLarge(\n",
    "    in_features=13,\n",
    "    dense_width=32,\n",
    "    num_classes=12,\n",
    "    input_exponent_bit_width=4,\n",
    "    input_mantissa_bit_width=3,\n",
    "    weight_exponent_bit_width=4,\n",
    "    weight_mantissa_bit_width=3,\n",
    ") # !:Large model\n",
    "\n",
    "# ---- Quantization method ----\n",
    "\n",
    "device = (\n",
    "    \"xpu\"\n",
    "    if torch.xpu.is_available()  # For the only person who has an Intel GPU\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6629564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 12 local datasets into global loaders.\n",
      "  Training samples: 652160\n",
      "  Test samples: 163040\n"
     ]
    }
   ],
   "source": [
    "# Train on global dataset\n",
    "train_loader, test_loader = get_global_smartpixel_dataloaders(\n",
    "    DATASET_PATH, pin_memory=True, pin_memory_device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5b96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local 0:\n",
      "  Training set shape : 45415 samples\n",
      "  Test set shape     : 11113 samples\n"
     ]
    }
   ],
   "source": [
    "# Train on local dataset, change local_id for different clients (0~11) \n",
    "train_loader, test_loader = get_smartpixel_dataloaders(\n",
    "    DATASET_PATH, pin_memory=True, pin_memory_device=device, local_id=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7102a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd8e40718ee42c9b35d631b571c111c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z.ling.865/smartpixel-brevitas/venv/lib/python3.10/site-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcd5565d57a4e3699d383d772a37e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaab55a34ea14cc4a3eea497c2c2c53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808fa299c9d34c5db18ad400619a0962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71db67a146524cec909c01eccc3f05c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32f6d22de734bcaa07bf0ec66917245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a1e129dff0471385c35bdaa0fbba2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a24a15030847d2a3d1779b52c5e181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c250aaba90349799de2b50cbccd0a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a67504dc7674abe87b3f6d86964e7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c896bee73f0e4651962c1ac334609831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b209b14232e64b81b29a9aae38d970d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b42075373bc4a6081c9926c161ca2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493ffacdb143452389939fde19b9bc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91890afc4ae420996c2a3f021e99c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3900ede5f1014b7b8493c01c83ae887e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8adcb5425049a88985127662a211cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7bdf37328443dc9edac242ea5a3f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80435912aadf40c1bfb5781dc311a49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f7f71124b644bf833f25c9d3bd0381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_for_epoch(model, device, train_loader, criterion, optimizer)\n",
    "    test_model(model, device, test_loader, F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d671b9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353e72be6356472995dd456695c9a027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all correctly classified images/inputs from test dataset\n",
    "correct_inputs, correct_targets = test_model(\n",
    "    model, device, test_loader, F.cross_entropy, return_correct=True\n",
    ")\n",
    "\n",
    "# Get quantized weights and inputs\n",
    "model_weights, model_scale = model.quant_weight()\n",
    "correct_inputs, input_scale = model.quant_input(correct_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c37e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting multi-layer quantized model weights...\n",
      "Exported model 'large_float_quant' to ./model_outputs/large_float_quant\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"large_float_quant\"  # Change me for different models\n",
    "EXPORT_PATH = os.path.join(EXPORT_PATH, MODEL_NAME)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(EXPORT_PATH):\n",
    "    os.makedirs(EXPORT_PATH)\n",
    "\n",
    "# Export NumPy for compatibility with Rust\n",
    "if isinstance(model_weights, dict):\n",
    "    print(\"Exporting multi-layer quantized model weights...\")\n",
    "    for layer_name, weights in model_weights.items():\n",
    "        np.save(EXPORT_PATH + f\"{layer_name}_weights.npy\", weights.cpu().numpy())\n",
    "\n",
    "    for layer_name, scale in model_scale.items():\n",
    "        np.save(EXPORT_PATH + f\"{layer_name}_scale.npy\", scale.cpu().numpy()) \n",
    "else:\n",
    "    print(\"Exporting single-layer model weights...\")\n",
    "    np.save(EXPORT_PATH + \"/model_weights.npy\", model_weights.cpu().numpy())\n",
    "    np.save(EXPORT_PATH + \"/model_scale.npy\", model_scale.cpu().numpy())\n",
    "np.save(EXPORT_PATH + \"/correct_inputs.npy\", correct_inputs.cpu().numpy())\n",
    "np.save(EXPORT_PATH + \"/input_scale.npy\", input_scale.cpu().numpy())\n",
    "np.save(EXPORT_PATH + \"/correct_targets.npy\", correct_targets.cpu().numpy())\n",
    "\n",
    "print(f\"Exported model '{MODEL_NAME}' to {EXPORT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7248b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), EXPORT_PATH + \"/smartpixel_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
